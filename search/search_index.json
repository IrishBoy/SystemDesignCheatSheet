{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Courses, Cheatsheet, Learning Plans from digitalbabushka.org \u00b6","title":"Courses, Cheatsheet, Learning Plans from digitalbabushka.org"},{"location":"#courses-cheatsheet-learning-plans-from-digitalbabushkaorg","text":"","title":"Courses, Cheatsheet, Learning Plans from digitalbabushka.org"},{"location":"Application%20Architecture/Introduction/","text":"First of all, this course is not one more \"How to pass System Design interview in MAANG\" or \"Excel your system design interviews\". This course is dedicated towards people who are new to IT or is looking forward broaden there horizons on the field of System Analysis or maybe who thinks that they lack of knowledge in IT. Most of the information in this course was taken from my personal knowledge database (well, for sure before appearing in it, it was either on some site or in one of my projects). So, cut the crap and let start. Number zero, despite the fact that I call it a \"course\", the better name is dictionary or check-list, or whatever. Since I mainly focus not on teaching things, but on giving things that should be learned. \u00b6 Firstly, there are two ways of going through this course Along, going from chapter to chapter as I modelled it Go to the chapters that interest you the most Secondly, Chapter Cases may be presumed as preparation for passing System Design interview, but I would't like to make it the main target of this course. Thirdly, it is not about writing code, it is about structuring logic, modelling architecture, sometimes unorthodox approaches and mostly about interaction between services, not inside service. That means this course is not about writing the code)","title":"Introduction"},{"location":"Application%20Architecture/Introduction/#number-zero-despite-the-fact-that-i-call-it-a-course-the-better-name-is-dictionary-or-check-list-or-whatever-since-i-mainly-focus-not-on-teaching-things-but-on-giving-things-that-should-be-learned","text":"Firstly, there are two ways of going through this course Along, going from chapter to chapter as I modelled it Go to the chapters that interest you the most Secondly, Chapter Cases may be presumed as preparation for passing System Design interview, but I would't like to make it the main target of this course. Thirdly, it is not about writing code, it is about structuring logic, modelling architecture, sometimes unorthodox approaches and mostly about interaction between services, not inside service. That means this course is not about writing the code)","title":"Number zero, despite the fact that I call it a \"course\", the better name is dictionary or check-list, or whatever. Since I mainly focus not on teaching things, but on giving things that should be learned."},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/5.%20Extra%20Materials/","text":"https://stackoverflow.com/questions/38024514/understanding-kafka-topics-and-partitions https://medium.com/javarevisited/kafka-partitions-and-consumer-groups-in-6-mins-9e0e336c6c00 https://redpanda.com/what-is-redpanda https://netflix.github.io/falcor/starter/what-is-falcor.html https://medium.com/javarevisited/difference-between-rabbitmq-apache-kafka-and-activemq-65e26b923114 https://medium.com/another-integration-blog/what-is-an-enterprise-service-bus-1991646764e2 https://eda-visuals.boyney.io/visuals/queues-vs-streams-vs-pubsub","title":"5. Extra Materials"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/0.%20Request%20Response/","text":"When using any application, you face one of two fundamental approaches Local app Web application The main difference between those types is the need of the internet connection to operate. Local apps, such as Calculator Camera etc. Do not send or receive information from internet in order to work properly When it comes to the apps that require internet, the following ones may be listed Weather forecast News Chess game Those apps use internet. Now, let's pick up the second category and break down the logic of their operating So, there is some logic on your device and some logic on the server But how those two parts communicate? Easy question - over the web If we go deeper, there is a list of protocols that are being used for this communication (Application layer of OSI Model ) HTTP - Hyper Text Transfer Protocol (or more secure HTTPS) FTP - File Transfer Protocol DNS - Domain Name System AMQP - Advanced Message Queuing Protocol SSH - Secure Shell etc. Request Response \u00b6 Request-response model is the main basic pattern when we talk about integrations. The logic behind it is very simple You make a request (call a method) You receive a response Which means that by default no other system can get your response, as well as the fact that initiator and target are known. Of course, we should mention here sync/asycn , but for now it is not that important","title":"0. Request Response"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/0.%20Request%20Response/#request-response","text":"Request-response model is the main basic pattern when we talk about integrations. The logic behind it is very simple You make a request (call a method) You receive a response Which means that by default no other system can get your response, as well as the fact that initiator and target are known. Of course, we should mention here sync/asycn , but for now it is not that important","title":"Request Response"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/1.REST/","text":"This is the most popular protocol, we will not spend to much time here This protocol utilises HTTP (S) 2nd or 3rd generation, it consists of the following things Method type - GET in this case GET - get info POST - add new info PUT - full update PATCH - partial update DELETE - delete OPTIONS - get available method types there are some more that are not that widely used URL request - *www.ya.com in this case Subdomain - api in this Endpoint - method in this case Query parameters - type=1 in this case Pagination parameters - page=2&limit=10 in this case Body \u00b6 For every request type (except for GET, OPTIONS and some others) you can have body Body is a payload of the request. What can be body? Well - application/xml - For xml markup - application/json - For JSON - image/png - For png - etc How to manage it ? It is managed in #Headers of the request, in Content-type field Headers \u00b6 Headers are additional data sent with a request to the server, providing key information like the content type of the request body, authentication tokens, or details about the client software. They are distinct from the request body and mainly serve to convey metadata about the request or response. What can store headers? Accept : Indicates the content type that the client expects in the response. Content-Type : Defines the media type of the request body. Authorization : Provides the authentication token or credentials for the request. User-Agent : Identifies the software client making the request. Cache-Control : Controls caching behavior for both the request and response. If-None-Match : Makes a conditional request based on the ETag header value.","title":"1.REST"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/1.REST/#body","text":"For every request type (except for GET, OPTIONS and some others) you can have body Body is a payload of the request. What can be body? Well - application/xml - For xml markup - application/json - For JSON - image/png - For png - etc How to manage it ? It is managed in #Headers of the request, in Content-type field","title":"Body"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/1.REST/#headers","text":"Headers are additional data sent with a request to the server, providing key information like the content type of the request body, authentication tokens, or details about the client software. They are distinct from the request body and mainly serve to convey metadata about the request or response. What can store headers? Accept : Indicates the content type that the client expects in the response. Content-Type : Defines the media type of the request body. Authorization : Provides the authentication token or credentials for the request. User-Agent : Identifies the software client making the request. Cache-Control : Controls caching behavior for both the request and response. If-None-Match : Makes a conditional request based on the ETag header value.","title":"Headers"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/2.gRPC/","text":"gRPC - Google Remote Procedure Call Works over HTTP , and gives more flexibility than REST , however there are not many libraries for front-end languages. And it uses protobuf protocol Each request consists of the following parts Request name Request body Response body syntax = \"proto3\"; import \"google/protobuf/timestamp.proto\"; service BillingService{ rpc GetBalance(GetBalanceRequest) returns (GetBalanceResponse); } message GetBalanceRequest{ repeated string account_ids = 1; } message GetBalanceResponse{ repeated Account accounts = 1; } message Account{ string user_id = 1; string account_id = 2; AccountType account_type = 3; int64 balance = 4; string currecny = 5; google.protobuf.Timestamp updated_at = 6; } enum AccountType{ ACCOUNT_TYPE_UNDIFINED = 0; ACCOUNT_TYPE_1 = 1; ACCOUNT_TYPE_2 = 2; ACCOUNT_TYPE_3 = 3; } service BillingService - could be described as package or group of methods GetBalance - method name GetBalanceRequest - Request, could be empty GetBalanceResponse - Response, also could be empty Each message (response or request) consists of several (may be no one) fields Each field can be set //String value string user_id = 1 ; //Array of int8 repeated int8 ids = 2 ; //Custom message type CustomMessage custom_message = 3 ; Why to use grcp instead of REST? \u00b6 No artificial restrictions in terms of method types Besides the fact that we can use GET, POST etc. as we wish, it still may bring some frustration More flexible types There are more variety of types, for example int4, timestamp etc. Speed You can see on the graph that gRPC is much faster on big data Here is the link","title":"2.gRPC"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/2.gRPC/#why-to-use-grcp-instead-of-rest","text":"No artificial restrictions in terms of method types Besides the fact that we can use GET, POST etc. as we wish, it still may bring some frustration More flexible types There are more variety of types, for example int4, timestamp etc. Speed You can see on the graph that gRPC is much faster on big data Here is the link","title":"Why to use grcp instead of REST?"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/3.graphQL/","text":"This API protocol also works over HTTP , however, it gives more flexibility in terms of requests. graphQL focuses on getting specific information that is needed, without any extra. The principle is based on making sql-like requests retrieving exact information you need For example, let's imagine that we have a service that is responsible for movies, directors etc. Case 1: You need to get director's birthdate, but in the director model there are 30 additional fields that you don't really need Case 2: You need to get name of directors fathers dog sister, with other API types you would need to make many requests, with graphQL you can extract required information with one request. Basic request example { # Object name hero { # Object parametrs name } } Response { \"data\": { \"hero\": { \"name\": \"R2-D2\" } } } It is also possible to filter Request { human(id: \"1000\") { name height } } Response { \"data\": { \"human\": { \"name\": \"Luke Skywalker\", \"height\": 1.72 } } } etc. More detailed information may be find here https://graphql.org/learn/queries/","title":"3.graphQL"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/4.Pub-sub%20vs%20message%20queue/","text":"Queues and pub/sub are another way of communication between applications. Imagine some sort of jar, someone puts there a paper with text, then another person comes and takes it. So, this is the main idea of queues, publisher puts some message into the queue, then subscriber reads it. Without any doubt we can configure the whole process according to the following parameters How many times message could be read Who can publish messages Whether the messages should be logged etc But for now it is not that important so, what's the difference? Feature Message Queue (MQ) Publisher-Subscriber (Pub-Sub) Concept of Message Messages generally have a specific destination. Messages, also known as events , are sent to multiple subscribers. Message Delivery Messages are delivered to one consumer. Messages are delivered to all subscribers. Event Retry Support exists, with the use of a dead letter queue . Depends on the queue implemented under the hood. Memory Requires significant amount of memory. Requires less memory as messages are discarded post distribution. Reliability Higher, thanks to the acknowledgement concept. Lower, due to lack of acknowledgment. Throughput Lower, due to the processing of a single message at a time. Higher, as multiple messages can be processed parallelly. Common Usage Task distribution, load balancing. Multi-casting events, data streaming.","title":"4.Pub sub vs message queue"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/5.kafka/","text":"Kafka is the most popular message queue in today's world It works over TCP Here you can see how it is structured So, what are partitions, topics, producer, consumer and consumer groups ? Producer - services that publishes messages to the broker Topic - category, or folder, or any analogy you can come up with, it is more logical unit that helps us to separate messages Partitions - every topic may be distributed across different nodes, those distributed units are called partitions Consumer and consumer group - those are services that are going to consume messages, the main difference is If two consumers have subscribed to the same topic and are present in the same consumer group, then these two consumers would be assigned a different set of partitions and none of these two consumers would receive the same messages.","title":"5.kafka"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/1.Web%20Sockets/","text":"So, what if you want to have bidirectional connection since you need lower latency ? Well, then you need web sockets Within this approach TCP is used and connection is up until client or server shuts it down. Step 1. The client initiates an HTTP connection. Step 2. WebSocket establishes a handshake between the client and server that makes WebSocket compatible with HTTP ports (80 and 443) and proxies. Step 3. Both the client and server exchange messages freely via an open, persistent connection that doesn\u2019t require continuous polling or new HTTP requests. Step 4. The connection remains open until one side closes the channel, at which point the connection closes and the client-server engagement ends. It is easy to understand pros but what are cons? Increased Complexity Resource Consumption Firewall and Proxy Issues Security Concerns Lack of Built-in Retry Mechanism Incompatible with REST APIs Monitoring and Debugging complexity No Native Support for Caching","title":"1.Web Sockets"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/2.Web%20hooks%20and%20Polling/","text":"Imagine a classic family road trip: Polling is like the kids constantly asking, \"Are we there yet?\" and the parents repeatedly responding, \"No.\" Webhooks are like the kids peacefully sleeping in the backseat, only being woken up by the parents when they've reached the destination. With webhooks, there's no need to keep checking for updates. Instead, the service provider sends the data to a specified URL as soon as it's available.","title":"2.Web hooks and Polling"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/3.RabbitMQ/","text":"RabbitMQ \u00b6 RabbitMQ is another example of message broker It works on AMQP Here you can observe architecture So, the main difference from kafka is the presence of Exchange : Receives messages from producers and routes them to queues based on the rules set by the exchange type. For a queue to receive messages, it must be bound to at least one exchange. Direct Exchange : Routes messages to queues based on an exact match between the message routing key and the queue's binding key. Fanout Exchange : Broadcasts messages to all queues bound to the exchange, ignoring the routing key. Topic Exchange : Routes messages to queues based on pattern matching between the routing key and the queue's binding key, allowing flexible and wildcard matching. Headers Exchange : Routes messages based on matching message header values rather than the routing key. Binding : A binding creates a connection between a queue and an exchange.","title":"3.RabbitMQ"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/3.RabbitMQ/#rabbitmq","text":"RabbitMQ is another example of message broker It works on AMQP Here you can observe architecture So, the main difference from kafka is the presence of Exchange : Receives messages from producers and routes them to queues based on the rules set by the exchange type. For a queue to receive messages, it must be bound to at least one exchange. Direct Exchange : Routes messages to queues based on an exact match between the message routing key and the queue's binding key. Fanout Exchange : Broadcasts messages to all queues bound to the exchange, ignoring the routing key. Topic Exchange : Routes messages to queues based on pattern matching between the routing key and the queue's binding key, allowing flexible and wildcard matching. Headers Exchange : Routes messages based on matching message header values rather than the routing key. Binding : A binding creates a connection between a queue and an exchange.","title":"RabbitMQ"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/4.%20SDK/","text":"SDK - software development kit This is a tool that allows you to communicate with some external system using your programming language. Sounds a bit frustrating, so, let's say that SDK == a couple of libraries Here you can see difference between API, Library and SDK What about examples ? \u00b6 Asana SDK \u00b6 // Install by running `npm i asana` const Asana = require ( 'asana' ); let asanaClient = Asana . ApiClient . instance ; let clientToken = asanaClient . authentications [ 'token' ]; clientToken . accessToken = '' ; let tasksApiInstance = new Asana . TasksApi (); let options = { \"limit\" : 50 , \"project\" : \"\" , }; const response = await tasksApiInstance . getTasks ( options ) console . log ( response ) This code allows to create a task in Asana without making any http request manualy","title":"4. SDK"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/4.%20SDK/#what-about-examples","text":"","title":"What about examples ?"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/4.%20SDK/#asana-sdk","text":"// Install by running `npm i asana` const Asana = require ( 'asana' ); let asanaClient = Asana . ApiClient . instance ; let clientToken = asanaClient . authentications [ 'token' ]; clientToken . accessToken = '' ; let tasksApiInstance = new Asana . TasksApi (); let options = { \"limit\" : 50 , \"project\" : \"\" , }; const response = await tasksApiInstance . getTasks ( options ) console . log ( response ) This code allows to create a task in Asana without making any http request manualy","title":"Asana SDK"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/5.%20ESB/","text":"Enterprise Service Bus - a huge thing for huge systems Basically it is the mechanism to keep all the components on the same side regarding the data Meaning that this is a mechanism to support real live data exchange between all the applications inside the system It is vital to say, that it is not a technology it is a pattern , this means that ESB can be implemented via endless array of different technologies. So, let's draw (obviously I've stolen this picture from some article) Here it is shown that different systems use ESB in order to get and send data across all other services. It is vital to state that this approach should only be used when the system may be called big and you have an enormous amount of data How you can implement it ? \u00b6 Message broker that supports fan out principles A set of services that pulls and updates the data from each service to each service etc What technologies to look at ? \u00b6 Apache Camel WSO2 Enterprise Integrator MuleSoft Anypoint Platform Talend ESB","title":"5. ESB"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/5.%20ESB/#how-you-can-implement-it","text":"Message broker that supports fan out principles A set of services that pulls and updates the data from each service to each service etc","title":"How you can implement it ?"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/5.%20ESB/#what-technologies-to-look-at","text":"Apache Camel WSO2 Enterprise Integrator MuleSoft Anypoint Platform Talend ESB","title":"What technologies to look at ?"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/1.RedPanda/","text":"Redpanda is a source available (BSL), Apache Kafka\u00ae-compatible, streaming data platform designed from the ground up to be lighter, faster, and simpler to operate. It employs a single binary architecture, free from ZooKeeper and JVMs, with a built-in Schema Registry and HTTP Proxy. No Java : A JVM-free and ZooKeeper-free infrastructure. Designed in C++ : Designed for a better performance than Apache Kafka. A single-binary architecture : No dependencies to other libraries or nodes. Self-managing and self-healing : A simple but scalable architecture for on-premise and cloud deployments. Kafka-compatible : Out-of-the-box support for the Kafka protocol with existing applications, tools, and integrations. Kafka vs Red Panda \u00b6 Kafka Red panda License Open source Under the Apache License governed by the Apache Software Foundation. Source available Under the Business Source License (BSL) with proprietary paid features available under an enterprise license. Contribution model and commercial backing Open Actively managed and maintained by 1,000+ full-time contributors at over a dozen companies and commercially backed by a broad coalition of vendors. Restricted Solely developed and maintained by Redpanda, with restrictive commercial support from other vendors due to BSL license agreement. Source Language Java C++ ZooKeeper Dependency No dependency ZooKeeper was removed by KRaft since version 3.3+ No dependency ZooKeeper-free and uses the Raft consensus algorithm. Storage Pattern and Performance Impact Consistent performance across most real-world workloads Kafka has a purpose-built log and replication layer optimized for sequential IO, which allows it to deliver high throughput and low latency across a broad set of hardware and workloads. Performance optimized for selective workloads Redpanda can demonstrate low latency and high throughput on simple workloads. However, because it\u2019s optimized for random IO, its performance can significantly degrade over time. Several common production configurations, such as high producer count, over 30% disk utilization, enabling message keys, enabling TLS, or running for more than 24 hours can cause severe reductions in performance. Broker Framework Purpose-built immutable log Uses its own purpose-built framework. Data is written in large blocks as high throughput sequential IO, allowing for high performance on drives with even very low IOPS. Based on Seastar Uses the Seastar framework, popularized by the Scylla Database, to implement its immutable log. Writes data in small 16kB chunks by default, requiring very high IOPS SSDs. Tiered Storage In Progress with KIP-405 Slated for early access in Kafka release 3.6. Requires Enterprise License Redpanda\u2019s tiered storage requires the purchase of an enterprise license. Replication Protocol Kafka replication (ISR) Replication is synchronous but data is written to disk asynchronously by design. Brokers don\u2019t need to fsync for correctness and have in-built data recovery and repair. Raft protocol Both replication and writing to disk are synchronous. Data must be written (fsynced) to disk synchronously, otherwise, it is possible to lose data during an election of a new leader. Cloud Network Optimized Optimized for cloud Follower Fetching enables clients to read data from follower replicas in the current AZ, avoiding cross-AZ network costs Cloud optimization in beta Follower fetching recently released in version 23.2. Connectors and Stream Processing Included Kafka Connect and Kafka Streams are packaged as part of the core open source Kafka offering. These two components allow you to connect applications and databases together and process data streams at scale. Not Included Not included with Redpanda. While Kafka Connect and Kafka Streams are compatible, they require you to configure, manage, and scale your own JVM-based applications and jobs. Breadth of adoption Vast developer ecosystem and community Apache Kafka is used by 100,000+ organizations, including 80% of F100 companies, including Goldman Sachs, Netflix and Uber Limited adoption and community Redpanda is used by thousands of organizations (undisclosed)","title":"1.RedPanda"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/1.RedPanda/#kafka-vs-red-panda","text":"Kafka Red panda License Open source Under the Apache License governed by the Apache Software Foundation. Source available Under the Business Source License (BSL) with proprietary paid features available under an enterprise license. Contribution model and commercial backing Open Actively managed and maintained by 1,000+ full-time contributors at over a dozen companies and commercially backed by a broad coalition of vendors. Restricted Solely developed and maintained by Redpanda, with restrictive commercial support from other vendors due to BSL license agreement. Source Language Java C++ ZooKeeper Dependency No dependency ZooKeeper was removed by KRaft since version 3.3+ No dependency ZooKeeper-free and uses the Raft consensus algorithm. Storage Pattern and Performance Impact Consistent performance across most real-world workloads Kafka has a purpose-built log and replication layer optimized for sequential IO, which allows it to deliver high throughput and low latency across a broad set of hardware and workloads. Performance optimized for selective workloads Redpanda can demonstrate low latency and high throughput on simple workloads. However, because it\u2019s optimized for random IO, its performance can significantly degrade over time. Several common production configurations, such as high producer count, over 30% disk utilization, enabling message keys, enabling TLS, or running for more than 24 hours can cause severe reductions in performance. Broker Framework Purpose-built immutable log Uses its own purpose-built framework. Data is written in large blocks as high throughput sequential IO, allowing for high performance on drives with even very low IOPS. Based on Seastar Uses the Seastar framework, popularized by the Scylla Database, to implement its immutable log. Writes data in small 16kB chunks by default, requiring very high IOPS SSDs. Tiered Storage In Progress with KIP-405 Slated for early access in Kafka release 3.6. Requires Enterprise License Redpanda\u2019s tiered storage requires the purchase of an enterprise license. Replication Protocol Kafka replication (ISR) Replication is synchronous but data is written to disk asynchronously by design. Brokers don\u2019t need to fsync for correctness and have in-built data recovery and repair. Raft protocol Both replication and writing to disk are synchronous. Data must be written (fsynced) to disk synchronously, otherwise, it is possible to lose data during an election of a new leader. Cloud Network Optimized Optimized for cloud Follower Fetching enables clients to read data from follower replicas in the current AZ, avoiding cross-AZ network costs Cloud optimization in beta Follower fetching recently released in version 23.2. Connectors and Stream Processing Included Kafka Connect and Kafka Streams are packaged as part of the core open source Kafka offering. These two components allow you to connect applications and databases together and process data streams at scale. Not Included Not included with Redpanda. While Kafka Connect and Kafka Streams are compatible, they require you to configure, manage, and scale your own JVM-based applications and jobs. Breadth of adoption Vast developer ecosystem and community Apache Kafka is used by 100,000+ organizations, including 80% of F100 companies, including Goldman Sachs, Netflix and Uber Limited adoption and community Redpanda is used by thousands of organizations (undisclosed)","title":"Kafka vs Red Panda"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/2.Short%20polling%20VS%20%20Long%20polling/","text":"Short Polling \u00b6 What is Short Polling? Short polling works like periodic check-ins between a web browser and a server. It's similar to texting a friend and repeatedly asking, \"Any updates?\" When to Use Short Polling? Short polling is best for situations where immediate updates aren't essential. For example, apps that refresh data periodically, like weather apps, can benefit from short polling. Advantages: Simplicity: Short polling is easy to implement, making it suitable for a variety of applications. Compatibility: It functions well with most server environments and doesn\u2019t need complex configurations. Disadvantages: Excessive Requests: Regular, frequent requests can cause unnecessary server load and increase network traffic. Latency: Since updates are only fetched during polling intervals, it can lead to delays in receiving new information. Long polling: \u00b6 What is Long Polling? Long polling is like keeping an open conversation between the browser and server, where the server waits to respond until there's new information to share, reducing the need for constant check-ins. When to Use Long Polling? Long polling is ideal for scenarios where real-time updates are important, but you want to avoid excessive requests. It works well for applications like messaging apps, where you need immediate updates without repeatedly asking, \"Any new messages?\" Advantages: Reduced Requests: The server keeps the connection open until new data is available, cutting down on unnecessary requests. Near Real-Time Updates: Long polling delivers updates faster than short polling, offering a more responsive user experience. Disadvantages: Resource Intensive: Holding long-lived connections can put a strain on server resources, particularly if the system isn't optimized for it. Connection Loss Latency: If the connection drops, there can be delays in re-establishing it, affecting the user experience.","title":"2.Short polling VS  Long polling"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/2.Short%20polling%20VS%20%20Long%20polling/#short-polling","text":"What is Short Polling? Short polling works like periodic check-ins between a web browser and a server. It's similar to texting a friend and repeatedly asking, \"Any updates?\" When to Use Short Polling? Short polling is best for situations where immediate updates aren't essential. For example, apps that refresh data periodically, like weather apps, can benefit from short polling. Advantages: Simplicity: Short polling is easy to implement, making it suitable for a variety of applications. Compatibility: It functions well with most server environments and doesn\u2019t need complex configurations. Disadvantages: Excessive Requests: Regular, frequent requests can cause unnecessary server load and increase network traffic. Latency: Since updates are only fetched during polling intervals, it can lead to delays in receiving new information.","title":"Short Polling"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/2.Short%20polling%20VS%20%20Long%20polling/#long-polling","text":"What is Long Polling? Long polling is like keeping an open conversation between the browser and server, where the server waits to respond until there's new information to share, reducing the need for constant check-ins. When to Use Long Polling? Long polling is ideal for scenarios where real-time updates are important, but you want to avoid excessive requests. It works well for applications like messaging apps, where you need immediate updates without repeatedly asking, \"Any new messages?\" Advantages: Reduced Requests: The server keeps the connection open until new data is available, cutting down on unnecessary requests. Near Real-Time Updates: Long polling delivers updates faster than short polling, offering a more responsive user experience. Disadvantages: Resource Intensive: Holding long-lived connections can put a strain on server resources, particularly if the system isn't optimized for it. Connection Loss Latency: If the connection drops, there can be delays in re-establishing it, affecting the user experience.","title":"Long polling:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/3.Apache%20Thrift/","text":"Apache Thrift is something like gRPC BUT it can use binary protocol/ JSON protocol, and more It has another IDL (Interface Definition Language) It doesn't use HTTP /2 as its transport protocol, but it can be adapted to use it How does it look like ? \u00b6 Well, first of all here is Thrift Architecture for some nerds In order to start working you need to define interfaces data structures Whole process may be find here // ProductService.thrift namespace java com.example namespace py example struct Product { 1: i32 id, 2: string name, 3: double price, // ... other fields ... } service ProductService { Product getProductByID(1: i32 productID), list<Product> searchProducts(1: string query), bool updateProductStock(1: i32 productID, 2: i32 quantity), // ... other methods ... } After that you can generate Server (example in Java) try { TServerTransport serverTransport = new TServerSocket ( 9090 ); TServer server = new TSimpleServer ( new Args ( serverTransport ). processor ( processor )); // Use this for a multithreaded server // TServer server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor)); System . out . println ( \"Starting the simple server...\" ); server . serve (); } catch ( Exception e ) { e . printStackTrace (); } And also handlers public class CalculatorHandler implements Calculator . Iface { private HashMap < Integer , SharedStruct > log ; public CalculatorHandler () { log = new HashMap < Integer , SharedStruct > (); } public void ping () { System . out . println ( \"ping()\" ); } public int add ( int n1 , int n2 ) { System . out . println ( \"add(\" + n1 + \",\" + n2 + \")\" ); return n1 + n2 ; } public int calculate ( int logid , Work work ) throws InvalidOperation { System . out . println ( \"calculate(\" + logid + \", {\" + work . op + \",\" + work . num1 + \",\" + work . num2 + \"})\" ); int val = 0 ; switch ( work . op ) { case ADD : val = work . num1 + work . num2 ; break ; case SUBTRACT : val = work . num1 - work . num2 ; break ; case MULTIPLY : val = work . num1 * work . num2 ; break ; case DIVIDE : if ( work . num2 == 0 ) { InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Cannot divide by 0\" ; throw io ; } val = work . num1 / work . num2 ; break ; default : InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Unknown operation\" ; throw io ; } SharedStruct entry = new SharedStruct (); entry . key = logid ; entry . value = Integer . toString ( val ); log . put ( logid , entry ); return val ; } public SharedStruct getStruct ( int key ) { System . out . println ( \"getStruct(\" + key + \")\" ); return log . get ( key ); } public void zip () { System . out . println ( \"zip()\" ); } }","title":"3.Apache Thrift"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/3.Apache%20Thrift/#how-does-it-look-like","text":"Well, first of all here is Thrift Architecture for some nerds In order to start working you need to define interfaces data structures Whole process may be find here // ProductService.thrift namespace java com.example namespace py example struct Product { 1: i32 id, 2: string name, 3: double price, // ... other fields ... } service ProductService { Product getProductByID(1: i32 productID), list<Product> searchProducts(1: string query), bool updateProductStock(1: i32 productID, 2: i32 quantity), // ... other methods ... } After that you can generate Server (example in Java) try { TServerTransport serverTransport = new TServerSocket ( 9090 ); TServer server = new TSimpleServer ( new Args ( serverTransport ). processor ( processor )); // Use this for a multithreaded server // TServer server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor)); System . out . println ( \"Starting the simple server...\" ); server . serve (); } catch ( Exception e ) { e . printStackTrace (); } And also handlers public class CalculatorHandler implements Calculator . Iface { private HashMap < Integer , SharedStruct > log ; public CalculatorHandler () { log = new HashMap < Integer , SharedStruct > (); } public void ping () { System . out . println ( \"ping()\" ); } public int add ( int n1 , int n2 ) { System . out . println ( \"add(\" + n1 + \",\" + n2 + \")\" ); return n1 + n2 ; } public int calculate ( int logid , Work work ) throws InvalidOperation { System . out . println ( \"calculate(\" + logid + \", {\" + work . op + \",\" + work . num1 + \",\" + work . num2 + \"})\" ); int val = 0 ; switch ( work . op ) { case ADD : val = work . num1 + work . num2 ; break ; case SUBTRACT : val = work . num1 - work . num2 ; break ; case MULTIPLY : val = work . num1 * work . num2 ; break ; case DIVIDE : if ( work . num2 == 0 ) { InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Cannot divide by 0\" ; throw io ; } val = work . num1 / work . num2 ; break ; default : InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Unknown operation\" ; throw io ; } SharedStruct entry = new SharedStruct (); entry . key = logid ; entry . value = Integer . toString ( val ); log . put ( logid , entry ); return val ; } public SharedStruct getStruct ( int key ) { System . out . println ( \"getStruct(\" + key + \")\" ); return log . get ( key ); } public void zip () { System . out . println ( \"zip()\" ); } }","title":"How does it look like ?"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/","text":"Apache Avro is a data serialization framework Basically, it is another way of serializing JSON Key features of Avro include: Compact Serialization : Avro uses a compact binary format f Schema-Based : Avro relies on schemas to define the structure of the data being serialized. The schema is stored alongside the data, allowing for schema evolution. This means you can change the schema over time (e.g., adding or removing fields) without breaking backward or forward compatibility. Interoperability : Since the schema is embedded with the data, any system that reads Avro-serialized data can interpret it without needing additional information, making it great for distributed systems. Typical Use Cases: \u00b6 Data Storage : Avro is often used with systems like Hadoop, Apache Kafka, and Apache Flink for storing large amounts of data efficiently. Data Interchange : Since Avro is language-neutral, it is widely used for cross-language RPC (Remote Procedure Calls) and message passing in distributed systems. It is commonly paired with other technologies like 5.kafka for message queuing, or Hadoop for big data storage and processing. How does it work? \u00b6 1. Basic Avro Schema and Serialization (Python) \u00b6 First, you define the Avro schema in JSON format. This schema will describe the structure of the data you're going to serialize. Avro Schema (JSON) \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } This schema describes a User record with three fields: name (string), age (integer), and email (nullable string with a default value of null ). Python Example (Serialization and Deserialization) \u00b6 Now, let's use Python to serialize and deserialize data based on this Avro schema. Install the avro package if you haven't already: pip install avro-python3 Example Python script: import avro.schema import avro.io import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) # Create an example user record user = { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data: { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) datum_reader = avro . io . DatumReader ( schema ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data: { deserialized_user } \" ) Output: \u00b6 Serialized data: b'\\x06John Doe8\\x1ejohn.doe@example.com' Deserialized data: {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'} 2. Avro with Nullable Fields \u00b6 Avro supports nullable fields using a union type . Here's an example where the email field can either be a string or null . Schema with Nullable Field (JSON) \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } Python Example (Handling Null Values) \u00b6 user_without_email = { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user_without_email , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data (without email): { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data (without email): { deserialized_user } \" ) Output: \u00b6 Serialized data (without email): b'\\x0cJane Doe2\\x00' Deserialized data (without email): {'name': 'Jane Doe', 'age': 25, 'email': None} 3. Writing Avro Data to a File (Python) \u00b6 In real-world applications, Avro data is often written to files, especially in big data processing. Here's an example of how to write serialized Avro data to a file. Writing Avro Data to a File: \u00b6 import avro.datafile import avro.io import avro.schema import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) users = [ { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" }, { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } ] with open ( \"users.avro\" , \"wb\" ) as avro_file : writer = avro . datafile . DataFileWriter ( avro_file , avro . io . DatumWriter (), schema ) for user in users : writer . append ( user ) writer . close () print ( \"Data written to users.avro\" ) Reading Avro Data from a File: \u00b6 with open ( \"users.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ()) for user in reader : print ( user ) reader . close () Output: \u00b6 {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'} {'name': 'Jane Doe', 'age': 25, 'email': None} 4. Schema Evolution Example \u00b6 One of Avro\u2019s powerful features is schema evolution . Let\u2019s say you add a new field to the schema but still want to read data serialized with the old schema. Original Schema (Old): \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" } ] } New Schema (Evolved): \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } Python Example (Handling Schema Evolution): \u00b6 If data was written with the old schema, you can still read it with the new schema that has the email field added. new_schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" new_schema = avro . schema . parse ( new_schema_json ) with open ( \"users_old.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ( new_schema )) for user in reader : print ( user ) reader . close () Output: \u00b6 ```plaintext {'name': 'John Doe', 'age': 28, 'email': None} {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"4.Avro"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#typical-use-cases","text":"Data Storage : Avro is often used with systems like Hadoop, Apache Kafka, and Apache Flink for storing large amounts of data efficiently. Data Interchange : Since Avro is language-neutral, it is widely used for cross-language RPC (Remote Procedure Calls) and message passing in distributed systems. It is commonly paired with other technologies like 5.kafka for message queuing, or Hadoop for big data storage and processing.","title":"Typical Use Cases:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#how-does-it-work","text":"","title":"How does it work?"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#1-basic-avro-schema-and-serialization-python","text":"First, you define the Avro schema in JSON format. This schema will describe the structure of the data you're going to serialize.","title":"1. Basic Avro Schema and Serialization (Python)"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#avro-schema-json","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } This schema describes a User record with three fields: name (string), age (integer), and email (nullable string with a default value of null ).","title":"Avro Schema (JSON)"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#python-example-serialization-and-deserialization","text":"Now, let's use Python to serialize and deserialize data based on this Avro schema. Install the avro package if you haven't already: pip install avro-python3 Example Python script: import avro.schema import avro.io import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) # Create an example user record user = { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data: { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) datum_reader = avro . io . DatumReader ( schema ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data: { deserialized_user } \" )","title":"Python Example (Serialization and Deserialization)"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output","text":"Serialized data: b'\\x06John Doe8\\x1ejohn.doe@example.com' Deserialized data: {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'}","title":"Output:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#2-avro-with-nullable-fields","text":"Avro supports nullable fields using a union type . Here's an example where the email field can either be a string or null .","title":"2. Avro with Nullable Fields"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#schema-with-nullable-field-json","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] }","title":"Schema with Nullable Field (JSON)"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#python-example-handling-null-values","text":"user_without_email = { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user_without_email , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data (without email): { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data (without email): { deserialized_user } \" )","title":"Python Example (Handling Null Values)"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output_1","text":"Serialized data (without email): b'\\x0cJane Doe2\\x00' Deserialized data (without email): {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"Output:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#3-writing-avro-data-to-a-file-python","text":"In real-world applications, Avro data is often written to files, especially in big data processing. Here's an example of how to write serialized Avro data to a file.","title":"3. Writing Avro Data to a File (Python)"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#writing-avro-data-to-a-file","text":"import avro.datafile import avro.io import avro.schema import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) users = [ { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" }, { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } ] with open ( \"users.avro\" , \"wb\" ) as avro_file : writer = avro . datafile . DataFileWriter ( avro_file , avro . io . DatumWriter (), schema ) for user in users : writer . append ( user ) writer . close () print ( \"Data written to users.avro\" )","title":"Writing Avro Data to a File:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#reading-avro-data-from-a-file","text":"with open ( \"users.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ()) for user in reader : print ( user ) reader . close ()","title":"Reading Avro Data from a File:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output_2","text":"{'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'} {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"Output:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#4-schema-evolution-example","text":"One of Avro\u2019s powerful features is schema evolution . Let\u2019s say you add a new field to the schema but still want to read data serialized with the old schema.","title":"4. Schema Evolution Example"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#original-schema-old","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" } ] }","title":"Original Schema (Old):"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#new-schema-evolved","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] }","title":"New Schema (Evolved):"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#python-example-handling-schema-evolution","text":"If data was written with the old schema, you can still read it with the new schema that has the email field added. new_schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" new_schema = avro . schema . parse ( new_schema_json ) with open ( \"users_old.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ( new_schema )) for user in reader : print ( user ) reader . close ()","title":"Python Example (Handling Schema Evolution):"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output_3","text":"```plaintext {'name': 'John Doe', 'age': 28, 'email': None} {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"Output:"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/","text":"Falcor is a JavaScript library developed by Netflix It uses JSON Key concepts of Falcor: Virtual JSON Model : Falcor presents your data as a JSON object, even if the actual data is spread across different services or databases. Path-Based Access : Instead of making multiple HTTP requests to various APIs, you access data via paths, much like accessing properties in a JavaScript object. Optimized Data Fetching : Falcor only fetches the data you need, minimizing over-fetching and under-fetching of data. Caching : Falcor has built-in client-side caching, reducing the need for redundant network requests. 1. Setting up a Falcor Model \u00b6 This example shows how to create a Falcor model to interact with a virtual JSON graph. const falcor = require ( 'falcor' ); const HttpDataSource = require ( 'falcor-http-datasource' ); // Creating a model connected to a Falcor server const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) // URL to your Falcor server }); 2. Fetching Data using Path \u00b6 In this example, you fetch data by specifying a path within your virtual JSON model. // Retrieve data from a specific path in the virtual JSON model model . get ([ 'movies' , 1234 , 'title' ]). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix' } } } }); Here, we are retrieving the title of a movie with ID 1234 by requesting the path ['movies', 1234, 'title'] . 3. Fetching Multiple Values \u00b6 You can request multiple values from different paths in a single call. model . get ( [ 'movies' , 1234 , 'title' ], [ 'movies' , 1234 , 'releaseYear' ], [ 'movies' , 5678 , 'title' ] ). then ( response => { console . log ( response . json ); // Output: // { // movies: { // 1234: { title: 'The Matrix', releaseYear: 1999 }, // 5678: { title: 'Inception' } // } // } }); 4. Setting Data in the Falcor Model \u00b6 You can also update data in the model using set() . model . set ({ path : [ 'movies' , 1234 , 'title' ], value : 'The Matrix Reloaded' }). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix Reloaded' } } } }); 5. Using Falcor Router on the Server-Side \u00b6 Here\u2019s an example of a simple Falcor Router on the server that serves data. const falcorExpress = require ( 'falcor-express' ); const Router = require ( 'falcor-router' ); const express = require ( 'express' ); const app = express (); // Define the Falcor route app . use ( '/model.json' , falcorExpress . dataSourceRoute (( req , res ) => { return new Router ([ { route : \"movies[{integers:ids}]['title', 'releaseYear']\" , get : ( pathSet ) => { const results = []; pathSet . ids . forEach ( id => { results . push ({ path : [ 'movies' , id , 'title' ], value : id === 1234 ? 'The Matrix' : 'Inception' }); results . push ({ path : [ 'movies' , id , 'releaseYear' ], value : id === 1234 ? 1999 : 2010 }); }); return results ; } } ]); })); app . listen ( 3000 , () => { console . log ( 'Falcor server running on port 3000' ); }); This sets up a Falcor router that responds to requests for movie titles and release years based on the provided movie IDs. 6. Combining Falcor with React \u00b6 You can use Falcor with React to fetch and display data. import React , { useEffect , useState } from 'react' ; import falcor from 'falcor' ; import HttpDataSource from 'falcor-http-datasource' ; const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) }); const MoviesList = () => { const [ movies , setMovies ] = useState ([]); useEffect (() => { model . get ([ 'movies' , [ 1234 , 5678 ], 'title' ]). then ( response => { setMovies ( response . json . movies ); }); }, []); return ( < div > { Object . keys ( movies ). map ( id => ( < div key = { id } > < h3 > { movies [ id ]. title } < /h3> < /div> ))} < /div> ); }; export default MoviesList ; In this example, the MoviesList component fetches movie titles with IDs 1234 and 5678 when the component loads, and displays them in the UI .","title":"5.Falcor"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#1-setting-up-a-falcor-model","text":"This example shows how to create a Falcor model to interact with a virtual JSON graph. const falcor = require ( 'falcor' ); const HttpDataSource = require ( 'falcor-http-datasource' ); // Creating a model connected to a Falcor server const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) // URL to your Falcor server });","title":"1. Setting up a Falcor Model"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#2-fetching-data-using-path","text":"In this example, you fetch data by specifying a path within your virtual JSON model. // Retrieve data from a specific path in the virtual JSON model model . get ([ 'movies' , 1234 , 'title' ]). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix' } } } }); Here, we are retrieving the title of a movie with ID 1234 by requesting the path ['movies', 1234, 'title'] .","title":"2. Fetching Data using Path"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#3-fetching-multiple-values","text":"You can request multiple values from different paths in a single call. model . get ( [ 'movies' , 1234 , 'title' ], [ 'movies' , 1234 , 'releaseYear' ], [ 'movies' , 5678 , 'title' ] ). then ( response => { console . log ( response . json ); // Output: // { // movies: { // 1234: { title: 'The Matrix', releaseYear: 1999 }, // 5678: { title: 'Inception' } // } // } });","title":"3. Fetching Multiple Values"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#4-setting-data-in-the-falcor-model","text":"You can also update data in the model using set() . model . set ({ path : [ 'movies' , 1234 , 'title' ], value : 'The Matrix Reloaded' }). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix Reloaded' } } } });","title":"4. Setting Data in the Falcor Model"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#5-using-falcor-router-on-the-server-side","text":"Here\u2019s an example of a simple Falcor Router on the server that serves data. const falcorExpress = require ( 'falcor-express' ); const Router = require ( 'falcor-router' ); const express = require ( 'express' ); const app = express (); // Define the Falcor route app . use ( '/model.json' , falcorExpress . dataSourceRoute (( req , res ) => { return new Router ([ { route : \"movies[{integers:ids}]['title', 'releaseYear']\" , get : ( pathSet ) => { const results = []; pathSet . ids . forEach ( id => { results . push ({ path : [ 'movies' , id , 'title' ], value : id === 1234 ? 'The Matrix' : 'Inception' }); results . push ({ path : [ 'movies' , id , 'releaseYear' ], value : id === 1234 ? 1999 : 2010 }); }); return results ; } } ]); })); app . listen ( 3000 , () => { console . log ( 'Falcor server running on port 3000' ); }); This sets up a Falcor router that responds to requests for movie titles and release years based on the provided movie IDs.","title":"5. Using Falcor Router on the Server-Side"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#6-combining-falcor-with-react","text":"You can use Falcor with React to fetch and display data. import React , { useEffect , useState } from 'react' ; import falcor from 'falcor' ; import HttpDataSource from 'falcor-http-datasource' ; const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) }); const MoviesList = () => { const [ movies , setMovies ] = useState ([]); useEffect (() => { model . get ([ 'movies' , [ 1234 , 5678 ], 'title' ]). then ( response => { setMovies ( response . json . movies ); }); }, []); return ( < div > { Object . keys ( movies ). map ( id => ( < div key = { id } > < h3 > { movies [ id ]. title } < /h3> < /div> ))} < /div> ); }; export default MoviesList ; In this example, the MoviesList component fetches movie titles with IDs 1234 and 5678 when the component loads, and displays them in the UI .","title":"6. Combining Falcor with React"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/8.ActiveMQ/","text":"ActiveMQ is another message broker like 5.kafka it may-be used as pub/sub and message queue (see 4.Pub-sub vs message queue ) It can work over OpenWire STOMP AMQP MQTT HTTP and supports SSL for encryption. Also it can scale from single to complex, multi-broker architectures Here is nice example of Kafka vs RabbitMQ vs ActiveMQ Here is more detailed architecture of ActiveMQ Paging: In ActiveMQ , paging occurs when the broker stores messages on disk after exceeding its memory limit. This allows message producers to continue sending messages without running out of memory. Once memory usage drops, the broker retrieves and delivers the paged messages. Paging helps manage large message volumes by preventing memory exhaustion, and it's configured by adjusting memory and disk usage limits like memoryLimit and storeUsage . Journal - a high-performance, sequential log used to store message data before it's written to more permanent storage, like a database. The journal ensures data durability by recording all incoming messages and transactions, allowing recovery in case of a failure. Core client API . This is a simple intuitive Java API that allows the full set of messaging functionality without some of the complexities of JMS. JMS client API The standard JMS API is available at the client side. JDBC (Java Database Connectivity) is an API in Java that enables applications to interact with relational databases. It provides methods to execute SQL queries, update records, and retrieve data from databases like MySQL, PostgreSQL, and Oracle. More detailed info you can find here https://activemq.apache.org/components/artemis/documentation/1.0.0/index.html","title":"8.ActiveMQ"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/9.Nats/","text":"Well, Nats is a bit different type, developers themselves call it message oriented middleware (we can say that it is some kind of ESB ) Here you can see a principle of NAT's work As you can observe, it is really simple message queue/ESB In NATS, messages are published to subjects\u2014hierarchical string values that subscribers monitor to receive messages. Known for its high throughput and low latency, NATS supports multiple message formats, including JSON, XML, and plain text. Here is architecture So, what are pros ? Lightweight and High Performance : Delivers excellent performance with minimal resource consumption. Delivery Guarantees : Provides both at-most-once and at-least-once messaging semantics. User-Friendly : Easy to deploy and manage, without requiring a dedicated cluster. Scalable : Designed to scale efficiently in distributed systems and microservices architectures.","title":"9.Nats"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/4.Cases/1.POST%20as%20GET/","text":"","title":"1.POST as GET"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/0.Introduction/","text":"Every application you has its own set of logic elements Authorisation Notification etc. Besides the fact that it is one application, all those logic parts may Be requested with different frequencies Require different data Be implemented with different technologies Require different states etc. When those restrictions are overwhelming, we can thing towards separating some parts or even creating separate applications. In a nutshell, this part will be dedicated to different ways logic can be (or should not be) separated.","title":"0.Introduction"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/1.Monolith/","text":"When you start creating any application, you should not make it difficult. You need to make it work right now, so you start writing code put everything into one application. Bravo ! You have just created a Monolithic Architecture. How? Well, before going deeper into monolith let's define what is it ? The most basic and strict characteristics of a monolith architecture may be squeezed down to the following points - Single executable program - Single instance - One database (we talk about one database of each type ) - Unified Build and Deployment - Communication within application Is it a good or a bad approach? This question is not right. Each approach has its own pros and cons, so let's list them Advantages Disadvantages The simplicity of starting to develop Undoubtedly, it is easy to create a couple of classes with an array of functions in each that complete some uncomplicated operations. Hard to support If the error is not a simple one like \u201cline 147 pages.py method length() is not callable\u201d, but anything more complicated, it is almost impossible to find the right place, where error occurred. Moreover, when a program does not fail, but just gives wrong results, you may face a vast array of difficulties trying to figure out part of the system, where it started to fail as everything is messed up and tightly coupled The easiness of testing Besides being the \u00abfirst\u00bb architecture design, it is without any doubt the one that provides the simplest way of testing, especially e2e testing as the only thing that is required is to run the program Whole system redeployment As a monolithic application is to be redeployed wholly, when one part fails or a new release is coming out, such a routine may drive a lot of people mad, and at the same time it has a severe effect on developing and down time No difficulties in deployment Almost always in such types of applications the only necessary thing is to write commands \u201cyarn start\u201d or \u201cpython file.py\u201d without any other parameters to configure. Parallel work is challenging As the system is tightly coupled, hardly can anyone argue that one part can be changed while the other is not finished, which makes projects/products that are based on this architecture design very slow and not let them use agile methodologies Moreover, when we talk about architecture and programming, it is vital to get into understanding two following things Cohesion and Coupling Why? Because usually bad monoliths are ones that have low cohesion, which led at some moment to appearing of new architecture styles that we will discuss in the following parts. For now we will stop here, however, later in more detailed version (so, when I have time), we will dive deeper into monolith and its types","title":"1.Monolith"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/2.SOA/","text":"Monolith is nice, it is easy to start, deploy and test and so one. However, it is also","title":"2.SOA"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/3.MSA/","text":"","title":"3.MSA"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/4.EDA/","text":"","title":"4.EDA"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/5.DDD/","text":"","title":"5.DDD"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/CQRS/","text":"","title":"CQRS"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/SAGA/","text":"","title":"SAGA"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/","text":"Microservices' structuring \u00b6 Methods naming \u00b6 Creating methods response logic \u00b6 Updating methods response logic \u00b6","title":"Microservices' structuring"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#microservices-structuring","text":"","title":"Microservices' structuring"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#methods-naming","text":"","title":"Methods naming"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#creating-methods-response-logic","text":"","title":"Creating methods response logic"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#updating-methods-response-logic","text":"","title":"Updating methods response logic"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/4.Authorisation/0.%20Authentication%20vs%20Authorisation/","text":"","title":"0. Authentication vs Authorisation"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/1.%20Cohesion%20and%20Coupeling/","text":"Cohesion \u00b6 Cohesion refers to how closely related and focused the responsibilities of a single module, class, or function are. Basically, this term refers to the measure how different parts of the same module is linked to each other - High cohesion : Modules or classes have a focused, single responsibility, making the code easier to understand, maintain, and modify. For example, a class that manages user authentication should only deal with tasks related to authentication (e.g., logging in, logging out, checking credentials). - Low cohesion : Modules or classes handle unrelated tasks, making the code more complex and harder to maintain. For example, a class that manages both user authentication and payment processing has low cohesion, because these are distinct responsibilities Coupling \u00b6 Coupling refers to the degree of direct dependence between different modules or components in a system. This measure evaluates how different parts of one module is connected with another module - Low coupling : Modules or components are independent and interact with each other through well-defined interfaces. This makes the system more flexible, allowing you to change or replace one part without significantly affecting others. - High coupling : Modules are tightly dependent on each other, which makes changes in one module likely to cause issues in others. This can lead to maintenance problems and make it harder to scale or refactor the system. So, we always should try to aim high cohesion low coupling","title":"1. Cohesion and Coupeling"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/1.%20Cohesion%20and%20Coupeling/#cohesion","text":"Cohesion refers to how closely related and focused the responsibilities of a single module, class, or function are. Basically, this term refers to the measure how different parts of the same module is linked to each other - High cohesion : Modules or classes have a focused, single responsibility, making the code easier to understand, maintain, and modify. For example, a class that manages user authentication should only deal with tasks related to authentication (e.g., logging in, logging out, checking credentials). - Low cohesion : Modules or classes handle unrelated tasks, making the code more complex and harder to maintain. For example, a class that manages both user authentication and payment processing has low cohesion, because these are distinct responsibilities","title":"Cohesion"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/1.%20Cohesion%20and%20Coupeling/#coupling","text":"Coupling refers to the degree of direct dependence between different modules or components in a system. This measure evaluates how different parts of one module is connected with another module - Low coupling : Modules or components are independent and interact with each other through well-defined interfaces. This makes the system more flexible, allowing you to change or replace one part without significantly affecting others. - High coupling : Modules are tightly dependent on each other, which makes changes in one module likely to cause issues in others. This can lead to maintenance problems and make it harder to scale or refactor the system. So, we always should try to aim high cohesion low coupling","title":"Coupling"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/4.Additional%20Materials/","text":"","title":"4.Additional Materials"},{"location":"Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/99.Additional%20Materials/","text":"","title":"99.Additional Materials"},{"location":"Application%20Architecture/2.Data/Data%20Vault/","text":"","title":"Data Vault"},{"location":"Application%20Architecture/2.Data/Hadoop/","text":"","title":"Hadoop"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/","text":"Relational Database \u00b6 Highlight: Excellent for structured data and complex queries, ensuring data integrity. Use Cases: Ideal for banking, CRM, and any scenario requiring strong ACID compliance. Examples: MySQL, PostgreSQL, Oracle. NoSQL Database \u00b6 Highlight: Great for scalability and flexibility with unstructured data. Use Cases: Suitable for big data analytics, real-time web apps, and content management. Examples: MongoDB, Cassandra, Redis. NewSQL Database \u00b6 Highlight: Combines traditional RDBMS ACID compliance with the scalability of NoSQL. Use Cases: Best for applications needing high transaction rates along with strong consistency, like financial trading platforms and high-speed retail systems. Examples: Google Spanner, CockroachDB, VoltDB. Document-Oriented Database \u00b6 Highlight: Stores data in document formats, offering schema flexibility. Use Cases: Best for content management systems, e-commerce platforms, and applications requiring frequent updates to the data structure. Examples: MongoDB, CouchDB, Amazon DocumentDB. Key-Value Database \u00b6 Highlight: Simple, efficient, and designed for high-speed read and write operations. Use Cases: Session management, caching, and scenarios where quick lookups are critical. Examples: Redis, DynamoDB, Etcd. Column-Oriented Database \u00b6 Highlight: Optimized for reading and writing data in columns, enhancing analytics and query performance. Use Cases: Big data processing, real-time analytics, and data warehousing. Examples: Cassandra, HBase, Google Bigtable. Object-Oriented Database \u00b6 Highlight: Aligns closely with object-oriented programming concepts, storing data as objects. Use Cases: Complex data models like CAD systems, AI applications, and simulation systems. Examples: db4o, ObjectDB, Versant. Time-Series Database \u00b6 Highlight: Specialized in handling time-stamped data, efficient in querying time-based data. Use Cases: IoT applications, financial services, and monitoring systems. Examples: InfluxDB, TimescaleDB, Kdb+. Wide-Column Store \u00b6 Highlight: Combines elements of relational and NoSQL, efficient for storing large volumes of data. Data warehousing, big data processing, and real-time analytics. Examples: Cassandra, Google Bigtable. Spatial Database \u00b6 Highlight: Specialized in storing and querying spatial information like maps and geographic locations. Ideal for geographic information systems (GIS), location-based services, and environmental modeling. Examples: PostGIS (extension for PostgreSQL), Oracle Spatial Graph Database \u00b6 Highlight: Optimized for storing and navigating complex relationships between data points. Use Cases: Social networks, recommendation engines, and fraud detection systems. Examples: Neo4j, Amazon Neptune, OrientDB. in-Memory Database \u00b6 Highlight: Stores data in the main memory (RAM) for faster processing speeds. Use Cases: High-performance applications like telecommunications, gaming, and real-time analytics. Examples: Redis, MemSQL.","title":"Relational Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#relational-database","text":"Highlight: Excellent for structured data and complex queries, ensuring data integrity. Use Cases: Ideal for banking, CRM, and any scenario requiring strong ACID compliance. Examples: MySQL, PostgreSQL, Oracle.","title":"Relational Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#nosql-database","text":"Highlight: Great for scalability and flexibility with unstructured data. Use Cases: Suitable for big data analytics, real-time web apps, and content management. Examples: MongoDB, Cassandra, Redis.","title":"NoSQL Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#newsql-database","text":"Highlight: Combines traditional RDBMS ACID compliance with the scalability of NoSQL. Use Cases: Best for applications needing high transaction rates along with strong consistency, like financial trading platforms and high-speed retail systems. Examples: Google Spanner, CockroachDB, VoltDB.","title":"NewSQL Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#document-oriented-database","text":"Highlight: Stores data in document formats, offering schema flexibility. Use Cases: Best for content management systems, e-commerce platforms, and applications requiring frequent updates to the data structure. Examples: MongoDB, CouchDB, Amazon DocumentDB.","title":"Document-Oriented Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#key-value-database","text":"Highlight: Simple, efficient, and designed for high-speed read and write operations. Use Cases: Session management, caching, and scenarios where quick lookups are critical. Examples: Redis, DynamoDB, Etcd.","title":"Key-Value Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#column-oriented-database","text":"Highlight: Optimized for reading and writing data in columns, enhancing analytics and query performance. Use Cases: Big data processing, real-time analytics, and data warehousing. Examples: Cassandra, HBase, Google Bigtable.","title":"Column-Oriented Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#object-oriented-database","text":"Highlight: Aligns closely with object-oriented programming concepts, storing data as objects. Use Cases: Complex data models like CAD systems, AI applications, and simulation systems. Examples: db4o, ObjectDB, Versant.","title":"Object-Oriented Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#time-series-database","text":"Highlight: Specialized in handling time-stamped data, efficient in querying time-based data. Use Cases: IoT applications, financial services, and monitoring systems. Examples: InfluxDB, TimescaleDB, Kdb+.","title":"Time-Series Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#wide-column-store","text":"Highlight: Combines elements of relational and NoSQL, efficient for storing large volumes of data. Data warehousing, big data processing, and real-time analytics. Examples: Cassandra, Google Bigtable.","title":"Wide-Column Store"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#spatial-database","text":"Highlight: Specialized in storing and querying spatial information like maps and geographic locations. Ideal for geographic information systems (GIS), location-based services, and environmental modeling. Examples: PostGIS (extension for PostgreSQL), Oracle Spatial","title":"Spatial Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#graph-database","text":"Highlight: Optimized for storing and navigating complex relationships between data points. Use Cases: Social networks, recommendation engines, and fraud detection systems. Examples: Neo4j, Amazon Neptune, OrientDB.","title":"Graph Database"},{"location":"Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#in-memory-database","text":"Highlight: Stores data in the main memory (RAM) for faster processing speeds. Use Cases: High-performance applications like telecommunications, gaming, and real-time analytics. Examples: Redis, MemSQL.","title":"in-Memory Database"},{"location":"Application%20Architecture/2.Data/999.Additional%20info/ACID/","text":"Atomicity All the writes in a transaction are executed as a single unit and cannot be divided into smaller parts. If any error occurs during the transaction, all changes are undone, ensuring that either all the writes are applied or none at all. This is why atomicity is often described as \"all or nothing.\" Consistency In this context, consistency refers to maintaining the integrity of the database by ensuring that all data adheres to the predefined rules and constraints. Unlike the CAP theorem's concept of consistency, which focuses on reading the latest write or returning an error, here it means that every transaction must leave the database in a valid state. Isolation When multiple transactions are running concurrently, isolation ensures that the operations of one transaction do not interfere with another. The strictest form of isolation is \"serializability,\" where each transaction behaves as though it is the only one being processed. Although serializability is difficult to achieve in practice, more relaxed isolation levels are commonly used. Durability Once a transaction is successfully committed, the data is guaranteed to be stored permanently, even in the event of a system failure. In distributed systems, durability often involves replicating the data across multiple nodes to ensure it remains intact.","title":"ACID"},{"location":"Application%20Architecture/2.Data/999.Additional%20info/Persistant%20Data/","text":"In the context of storing data in a computer system, this means that the data survives after the process with which it was created has ended. In other words, for a data store to be considered persistent, it must write to non-volatile storage. Which data stores provide persistence? \u00b6 If you need persistence in your data store, then you need to also understand the four main design approaches that a data store can take and how (or if) these designs provide persistence: Pure in-memory, no persistence at all, such as memcached or Scalaris In-memory with periodic snapshots, such as Oracle Coherence or Redis Disk-based with update-in-place writes, such as MySQL ISAM or MongoDB Commitlog-based, such as all traditional OLTP databases (Oracle, SQL Server, etc.) In-memory approaches can achieve blazing speed, but at the cost of being limited to a relatively small data set. Most workloads have relatively small \"hot\" (active) subset of their total data; systems that require the whole dataset to fit in memory rather than just the active part are fine for caches but a bad fit for most other applications. Because the data is in memory only, it will not survive process termination. Therefore these types of data stores are not considered persistent.","title":"Persistant Data"},{"location":"Application%20Architecture/2.Data/999.Additional%20info/Persistant%20Data/#which-data-stores-provide-persistence","text":"If you need persistence in your data store, then you need to also understand the four main design approaches that a data store can take and how (or if) these designs provide persistence: Pure in-memory, no persistence at all, such as memcached or Scalaris In-memory with periodic snapshots, such as Oracle Coherence or Redis Disk-based with update-in-place writes, such as MySQL ISAM or MongoDB Commitlog-based, such as all traditional OLTP databases (Oracle, SQL Server, etc.) In-memory approaches can achieve blazing speed, but at the cost of being limited to a relatively small data set. Most workloads have relatively small \"hot\" (active) subset of their total data; systems that require the whole dataset to fit in memory rather than just the active part are fine for caches but a bad fit for most other applications. Because the data is in memory only, it will not survive process termination. Therefore these types of data stores are not considered persistent.","title":"Which data stores provide persistence?"},{"location":"Application%20Architecture/5.Network/DNS/","text":"","title":"DNS"},{"location":"Application%20Architecture/5.Network/Firewall/","text":"","title":"Firewall"},{"location":"Application%20Architecture/5.Network/OSI%20Model/","text":"","title":"OSI Model"},{"location":"Application%20Architecture/5.Network/Proxy/","text":"","title":"Proxy"},{"location":"Application%20Architecture/5.Network/Protocols/1.Quik/","text":"","title":"1.Quik"},{"location":"Application%20Architecture/5.Network/Protocols/AMQP/","text":"","title":"AMQP"},{"location":"Application%20Architecture/5.Network/Protocols/FTP/","text":"","title":"FTP"},{"location":"Application%20Architecture/5.Network/Protocols/HTTP/","text":"","title":"HTTP"},{"location":"Application%20Architecture/5.Network/Protocols/SSH/","text":"","title":"SSH"},{"location":"Application%20Architecture/5.Network/Protocols/TCP/","text":"","title":"TCP"},{"location":"Application%20Architecture/Additional%20materials/%28A%29Sync/","text":"","title":"(A)Sync"},{"location":"Application%20Architecture/Additional%20materials/ETag/","text":"An ETag (Entity Tag) is a unique identifier assigned by a web server to a specific version of a resource, typically used for caching and conditional requests. When a resource (like a webpage, image, or file) is served, the server generates an ETag and sends it in the ETag header of the response. If the client caches the resource, it also stores the ETag. When the client later requests the same resource, it can include the If-None-Match header with the ETag value. This tells the server to return the resource only if it has changed (i.e., if the ETag doesn't match). If the resource is unchanged, the server responds with a 304 Not Modified status, allowing the client to use its cached version, which saves bandwidth and reduces load times","title":"ETag"},{"location":"Application%20Architecture/Additional%20materials/JSON/","text":"","title":"JSON"},{"location":"Application%20Architecture/Additional%20materials/Raft%20protocol/","text":"","title":"Raft protocol"},{"location":"Application%20Architecture/Additional%20materials/UI/","text":"","title":"UI"},{"location":"Application%20Architecture/Cases/Cases/","text":"","title":"Cases"},{"location":"Application%20Architecture/Excalidraw/Monolith.excalidraw/","text":"\u26a0 Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. \u26a0 You can decompress Drawing data with the command palette: 'Decompress current Excalidraw file'. For more info check in plugin settings under 'Saving' Excalidraw Data \u00b6 Text Elements \u00b6 Business Logic ^1QRHB8I5 Background Processes ^Eon269LW Data Interfaces ^0SqifRjG User inerface ^trbkeBCl And many more ^iUxlXOrw Single application ^kFrL3V5m %% Drawing \u00b6 N4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATLZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHALRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUebQBmbQAGGjoghH0EDihmbgBtcDBQMBLoeHF0QOwojmVg1JLIRhZ2LjR4gDYk/lLm1k4AOU4xbgBGePiAdgAOaaT4pIBWHshC DmIsbghcFJWIQmYAEXSoBGJuADMCML2SLYBZAC1JAAVFgFUAGQB9Z3iAKQAYhRRqMABK4OAAUSgACUGqULoR8PgAMqweoSQQeBECKCkNgAawQAHUSOpuHxCniCcT0TBMehsXc9gS/JIOOFcmhRns2HBcNg1DAxklutTtrVGeLGhBMNxnKNFpNkvFRjxlhKRWg/kkVdN4osAJxTTWy5j4okIADCbHwbFIWwAxKMEK7XbiIJpBYTlGz1rb7Y6JPjrM wBYFsp6KOTJJSACwdbTTI3xxZqjV7SQIQjKaTcePTFULPXTdVm0phM7cDozRY8eOTI17P3COAASWI3NQeQAunsLuRMp3uBwhCjWcJ1pzmN3irLYIhuPFqQBfPaaKfEKHBTLZbt9vZCODEXCnc48yY8I0dI2TDqjPWTeJ7IgcQmj8f4V9sbDEi+oFc+BhIU66FPOkCLhUEBVDUdRIHsfStGM8RUrKSGDMMFSjCaSRpos0wVqs6ybBIuDxJ6BzHME5 6XNcCC3ABEAAPKEKiFxGvoRi+AAKmChCYNazgkt8ADi7YAFYAIqekiKL0oyEDMucewWrSpKxpSqmWnSGLQcpnpsnmM7dryEr8oKwqijKpTWPB3A2ZA8o6qM8ZxPeoyTKMRZERA2qoIqd7JrW3mTL5alWoGDrOu6boIRK3p/q2QgBna0UhuQHDhrgkZQNGmloDwOHaEVSQmmFWY5nmeVoMq2gdPGSQdJmEpVgBqEdNMPA8DML4SslHZdvk/YSoOuD DgBY4ThK/rECZ3AQZB5TLmuG5bjuGRZDk+TUkUu1LUuIZYHlKylHcEijFJsJggAQtM7aLBAu2rtSI2ysep60Ze163mFRp4Wqr5rB+aBTd+Er2n+1ZoEBIElGBJQQWUh3oKcmAnRKGFtLwZnoUw/QcEMHAjDyt6po+ha42dJHOeguDxpRRwnNDgH0YxWyXddd0PbJyJonpWwGdp6lksQFKFcLVoKfpdosjNwjGVyYx8gKQqwNZex2dKey04qeHJB0 D6TM+nWjA1TV7P5ipltoiyJj5kvElFwboC6cUehuPrJalQZbKGWURlt+Vi3GPLxqMyY4YRjadKhZZoaU2a5vmhXhQgLPple/0LH1soDZ2B5vYiQ4ICOoNfpOKVzUr5fTbKm5Vxte7bWgh4Sh9Z4s55P13saANU5Ab4g6gYM/lDAGwwxo2cFAqKEEYFQ8I5EAXDPgLjci/m+WjNXoDdQisCZqCfGwqjYIZlA8cdWz74fXLH6f5iejvACCRDKNjEBi NkTCes0UDmAIG/XMn99AkGIPUPYehsi4DWEwMu6BWLsU4txIQfEBJCREuJaSnoHS5jWAQK+6Mb4HzgbOB+Z9PS4CEFANgsJwjzwqPiIQU9ZRvgQGCKqKdUARw1KBHoSMoJbFglreKeMWicEpEsRC+NWhExJjwwiJY5jxmbBKNYGxabbEercJmNEWaT3ZhIaYFwJIv3iPoZiUk2KPHoHAQ41pATPhfosAAGrzeSAssSyxUq1HSGkQ5aT8epaWgsfG GQVhyGuPCVaWXVjyMUmspQVGXrrVy7l5jlktgqUEix6pGiNEVB2wTIppRdhAN2sVPSJV9LNZ2ftMrZVysHcWOMjQlUfOVXySdqo1mXm1MYyoeAGnrEaXy+chqtyLpAMaE1Px11KLNeaaBFrIwqCuRoCNSgN3WE3LaB5dqLQOtBHeuIzpMShJwHgt5Pgkieo0MAL1GjTIgB3L6PCs6/UWHhZqudShD3meDNhv5/x0WAggfh4EJRCKOsQmREjsaPl8 ljeR2FExeW+fGVRtwaZbFwB0Rm1EEDvMMeoi5Vybl3IHHzUJ3icSOwCa0hONIpZeKZOEyuitZzK3MqrKyCTl6iIcjrBUKYSp3i6JMNyoVfJW1BEkfJhSZUMvqRISpcVqlezqWUhpYZA5Rj2DGQJPJ1QJAKf9Hg8R4ymkqsnXe3zVLpwAuTDonRJUtjZINQuA4S4INHvLKuyyR4VwSutXc+zhpHhPJ3AC3cby9yaksSYQN3yArHqCmGbNp7ZDngvK RA414b3wFvPYpyJA3S1VuAAOhwF4BIxCznCBfCgRDd4QArUlWaqA62/i5E20tx1gEfy2N/U4jp4UAPcEO0B4DIESmgVEOBpAEEQBMWYixVibF2IcU4+ILj3F8lIPgjghDr7lsrVXbt9a+25E1jQuhDC81oGYaw/5cDOF2rGCVRYkLEbQuWhIERyS/6yMke0PC8KCaospOqK1oITS+Q0aROmkxCXMwnlm2U510ADEkKiQ42A4AcEBJwR4FaJJGmIP 8a0hAOiAg8fzBkMt6UlOJKLJlDLaXspY7KIyUTuU8liWrfyj5BXAbQKknJ4cEioUyS1WUVsJi22auFfxqrXaxQ9glC9Pt0qo0afqjGsojVMuKqVLptreloC6I6ruSRrmzA6PWZN/VPUF0jaNX1k0Q28a3EG1ZMLUAbPhmtRu4b9w7QeUctZftr6nVWExJIqIACOhALiwgkqJe5mzXpRs+l3T5vcl5Lz+YPYGaaIYgoMfRX9e0FwAdRnFzGoHsbdW ZQwFr0GTUNkWM1UEA99i4rItMND+iMPgqMegJLqX0uZYY1xpSHLWOMtDrwTjbLFs8cWZEoNA2LLCY1hKIVEmRUuU8ske8qjLVlWVJk7JLkkxNXk5WNTOq1WabEdsnTxB1PQAMzlIOhqCofIVWTQiN4cKuSe5Z7hz2BBOrGIRV1PXQQerbO5qZPrxql28wsyASzon+vrmGzaEXMft2je8uNXzGpzFKxAAFtcgX/Kq+Nm42bZ6MPzRz9eYDi3cG3me 9Ahwzy4FQO2H+pArgNvQKyS+QuIAi6iOLyX0vwiy+hYO9+n9R2/wnYA/A06thgOIBAz7kAF2wM5MupiuH8OEeI6R8jlHqO0fo4e49p7iESCV2LiXY61fMA17KahtD6GsCfagF9KaOFcN3rwn9IWoX1ZRjBBA1QhWQeQoVeYWfMLE2whqRMt41QDaQ1o3AhxRvEuqxNslWxDhTFRO8WEepvhuJgFAb4HQBhGnuMQT4uxRo0o20LZb7HVvtYirpJjY Stv4529EvbfL4k8MSUd8TqBJMuSxbbUEXlikKYVNcpM8RUzpiyct376qtP12+79/2TTAcShM5PsznSbUSh6dwuq5sVO2YAiXiNGmC6EWG+XpwmW9U82xz9R8220DWiQCwa2C0eVC12XCxbh7EOX2hi1hSMweX2CYnxE0GJBumtHwGy3hlywp3y1jUKz+jtiSGmBTWHiJxZ3HjBThlQOT1KEC2gCa3EQJlFGYOawRS614DKgKQfHGBcywyGzpihGr xJUw3OQaRIIQDIIoOpU8VnzpTlnNH8QnyCQMJCVHyW183ZF2yE35TXzE3shOwlF1gKWTAhyLDCgbEImGXuwCg1AVT1GlUPxe3Umvw+01U7S3Af3+2aSB2NR4WvG0DvGALCkxTvCWG6Tj0pDTi7niBAKlVTD1DR2PAxx7BeVmRxwqwsOnEJzgMgB2W3AwKgPekpwKx7nvDFByPa0Z2DTxwZ1Z04NfRmRnlzUXmXlXmyF503gFwHW93QHeDCFIFQCX TV2bVbS2DmKYEWOt2WOmKgCNwkF13HVEMnSAW12N1nXNy/hnit3gSYkb0mGb1b0mHb1cU72717370H1wSPX8C9zbXWIWKWMFAuNDwfQjyYVIBYRjw/Ssx4W/Vq0EQazTwz2Azz2xjg1RPEOkMmCSCVHTBxU0TxXhF0SJWULrywyYh4AAGlnBJAYAeIOAOgpJmJ3gAA1DgCST4ZLdsG6Fkwkf4ebMw+fJSQw4HKffxBbMfCwrlUyaw1fUTJJewrfU 7AKcYaYbQe8MZDMWVZcNUw0O2TqCqK/N7DTd2C4mpb2H7Y0v7PVAHA1F/UU9/MqT/WUb/XeGzVqBHMOMUJzVyZ8Qor1DzWUMo2AnognATVAJAlGFArZWoknZuA5KLHAvg05eLQgrYQgd4TAfAVxZiUgKgZ6agpo2gsYegroCYO2Fgio9gjNVmcFeE/9VPFM0QoQwqP05suRLCBMdUDFW8fE5DbYVEJQ2vdnck9MzM7M3MqgbQxjRSSUoIq0IwiWZ bCU8w+A6UnlWUfbGw+UjfRU7fFUsUbQCmHEo0M2cA/6DobwsA5MMA+2Q0kw0pX2d7U0sI2pCIq0x/QzFpVbVyCOIsApcYCYMUGYdUGHXeOHJST01AfItMLyKYf04otuIMrzKshfBA8Mtg2MsLUnTApC0oN5Fo+NNossVCSspndNYcgYleIYrnHPAtcYotEtTXGYiAF+dYVAfQawGADih0C48gFtBXNi4gDiriniwIZ+LXEBEdLaPXI4g3PY9AE3M 3T0S3JdFdKkmkukhkpk1k9kzk7k3k/kj3H4/AVYiQISkSjgbi/QXiqhe9cPWiqPCEqi9haE7hBPeslPE5AQ3oFrSkB8DEzsnkBYNMA0oqPsivC4IctnKi7DCAQkQEUgT4eIFkxYfQAU3Q7jfQ+ctjUU9bTKzbbKtC9cwTXlOJETdfEPTffcxUbqZMM/aYa1e80oRTRqeqMKECiC6fG0K0m/M0+/D8qI5/YzYHTyNUjq+OdIz9VOAAsYf6JYHI/8h CyZEorHOZcigNKojCmor0OMiNcnIsmNEs1opzRqnIsi7o5nQePozNMkxEGiyPJeeiqACY/nWqHYrYOeZJVASEHwQBABSROXASlir6+CH6uAP6vAAGrgHYhShnBAC4fApoJgY4w3U4kMfkFSq4tS3HK6iAPBEysy9AUG4IcGyGs8ZCO9MPR9CoUlNhd9DInkOEqgsAN6bYCG9ETuBaQoaAbMTILYd+fMHoBgQgBACgDtN8qua/C4GW2WhEL+EQXKd sU4fQdEYI3q0I4W7ARWraZWjICWi0yIm06Inm7W0gJWlWwEEfAquchW823WlWtWhcvK02nW7IPW1W8UwU7Ku2i2jIWERfcMqmX2h2jIZiFfCqmyEO92y2wtPnJi0oM2v2/QEjHNRyp612+2mOjIVtOG4IRG+WpO0Oz2nKXY82tgCgbMXAXGrWt2qAD2qEdYF+cuyukIJiHKAkKcnm5gbAAkFEdxQqS1YKeNEAyVdJEQysXuu0fAAATWXAIiPKvGt RzmVFcj+QgCMDYAMG5vQgIBYS/TClq2jvrpVoDvQu7Bgi3Hlr9BIGGO51KBvstKfIjIlBujtCYidGtCNC/q/s9HoWUHHBymdChEOBAZAeyy2WPqduJHDuhu7DYIgDgECDMGEGYFEiPWIDvo2uLmx3oRIiPVqB3tKCyFwE0GCAAmj3nSIDgG4EodlBPX5ufWcr5BoXYVoecqPrsAknTxyFRBPTgHuDYA2EbtIfIf6PAARhXmRCbRWRelXCAA= %%","title":"Monolith.excalidraw"},{"location":"Application%20Architecture/Excalidraw/Monolith.excalidraw/#excalidraw-data","text":"","title":"Excalidraw Data"},{"location":"Application%20Architecture/Excalidraw/Monolith.excalidraw/#text-elements","text":"Business Logic ^1QRHB8I5 Background Processes ^Eon269LW Data Interfaces ^0SqifRjG User inerface ^trbkeBCl And many more ^iUxlXOrw Single application ^kFrL3V5m %%","title":"Text Elements"},{"location":"Application%20Architecture/Excalidraw/Monolith.excalidraw/#drawing","text":"N4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATLZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHALRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUebQBmbQAGGjoghH0EDihmbgBtcDBQMBLoeHF0QOwojmVg1JLIRhZ2LjR4gDYk/lLm1k4AOU4xbgBGePiAdgAOaaT4pIBWHshC DmIsbghcFJWIQmYAEXSoBGJuADMCML2SLYBZAC1JAAVFgFUAGQB9Z3iAKQAYhRRqMABK4OAAUSgACUGqULoR8PgAMqweoSQQeBECKCkNgAawQAHUSOpuHxCniCcT0TBMehsXc9gS/JIOOFcmhRns2HBcNg1DAxklutTtrVGeLGhBMNxnKNFpNkvFRjxlhKRWg/kkVdN4osAJxTTWy5j4okIADCbHwbFIWwAxKMEK7XbiIJpBYTlGz1rb7Y6JPjrM wBYFsp6KOTJJSACwdbTTI3xxZqjV7SQIQjKaTcePTFULPXTdVm0phM7cDozRY8eOTI17P3COAASWI3NQeQAunsLuRMp3uBwhCjWcJ1pzmN3irLYIhuPFqQBfPaaKfEKHBTLZbt9vZCODEXCnc48yY8I0dI2TDqjPWTeJ7IgcQmj8f4V9sbDEi+oFc+BhIU66FPOkCLhUEBVDUdRIHsfStGM8RUrKSGDMMFSjCaSRpos0wVqs6ybBIuDxJ6BzHME5 6XNcCC3ABEAAPKEKiFxGvoRi+AAKmChCYNazgkt8ADi7YAFYAIqekiKL0oyEDMucewWrSpKxpSqmWnSGLQcpnpsnmM7dryEr8oKwqijKpTWPB3A2ZA8o6qM8ZxPeoyTKMRZERA2qoIqd7JrW3mTL5alWoGDrOu6boIRK3p/q2QgBna0UhuQHDhrgkZQNGmloDwOHaEVSQmmFWY5nmeVoMq2gdPGSQdJmEpVgBqEdNMPA8DML4SslHZdvk/YSoOuD DgBY4ThK/rECZ3AQZB5TLmuG5bjuGRZDk+TUkUu1LUuIZYHlKylHcEijFJsJggAQtM7aLBAu2rtSI2ysep60Ze163mFRp4Wqr5rB+aBTd+Er2n+1ZoEBIElGBJQQWUh3oKcmAnRKGFtLwZnoUw/QcEMHAjDyt6po+ha42dJHOeguDxpRRwnNDgH0YxWyXddd0PbJyJonpWwGdp6lksQFKFcLVoKfpdosjNwjGVyYx8gKQqwNZex2dKey04qeHJB0 D6TM+nWjA1TV7P5ipltoiyJj5kvElFwboC6cUehuPrJalQZbKGWURlt+Vi3GPLxqMyY4YRjadKhZZoaU2a5vmhXhQgLPple/0LH1soDZ2B5vYiQ4ICOoNfpOKVzUr5fTbKm5Vxte7bWgh4Sh9Z4s55P13saANU5Ab4g6gYM/lDAGwwxo2cFAqKEEYFQ8I5EAXDPgLjci/m+WjNXoDdQisCZqCfGwqjYIZlA8cdWz74fXLH6f5iejvACCRDKNjEBi NkTCes0UDmAIG/XMn99AkGIPUPYehsi4DWEwMu6BWLsU4txIQfEBJCREuJaSnoHS5jWAQK+6Mb4HzgbOB+Z9PS4CEFANgsJwjzwqPiIQU9ZRvgQGCKqKdUARw1KBHoSMoJbFglreKeMWicEpEsRC+NWhExJjwwiJY5jxmbBKNYGxabbEercJmNEWaT3ZhIaYFwJIv3iPoZiUk2KPHoHAQ41pATPhfosAAGrzeSAssSyxUq1HSGkQ5aT8epaWgsfG GQVhyGuPCVaWXVjyMUmspQVGXrrVy7l5jlktgqUEix6pGiNEVB2wTIppRdhAN2sVPSJV9LNZ2ftMrZVysHcWOMjQlUfOVXySdqo1mXm1MYyoeAGnrEaXy+chqtyLpAMaE1Px11KLNeaaBFrIwqCuRoCNSgN3WE3LaB5dqLQOtBHeuIzpMShJwHgt5Pgkieo0MAL1GjTIgB3L6PCs6/UWHhZqudShD3meDNhv5/x0WAggfh4EJRCKOsQmREjsaPl8 ljeR2FExeW+fGVRtwaZbFwB0Rm1EEDvMMeoi5Vybl3IHHzUJ3icSOwCa0hONIpZeKZOEyuitZzK3MqrKyCTl6iIcjrBUKYSp3i6JMNyoVfJW1BEkfJhSZUMvqRISpcVqlezqWUhpYZA5Rj2DGQJPJ1QJAKf9Hg8R4ymkqsnXe3zVLpwAuTDonRJUtjZINQuA4S4INHvLKuyyR4VwSutXc+zhpHhPJ3AC3cby9yaksSYQN3yArHqCmGbNp7ZDngvK RA414b3wFvPYpyJA3S1VuAAOhwF4BIxCznCBfCgRDd4QArUlWaqA62/i5E20tx1gEfy2N/U4jp4UAPcEO0B4DIESmgVEOBpAEEQBMWYixVibF2IcU4+ILj3F8lIPgjghDr7lsrVXbt9a+25E1jQuhDC81oGYaw/5cDOF2rGCVRYkLEbQuWhIERyS/6yMke0PC8KCaospOqK1oITS+Q0aROmkxCXMwnlm2U510ADEkKiQ42A4AcEBJwR4FaJJGmIP 8a0hAOiAg8fzBkMt6UlOJKLJlDLaXspY7KIyUTuU8liWrfyj5BXAbQKknJ4cEioUyS1WUVsJi22auFfxqrXaxQ9glC9Pt0qo0afqjGsojVMuKqVLptreloC6I6ruSRrmzA6PWZN/VPUF0jaNX1k0Q28a3EG1ZMLUAbPhmtRu4b9w7QeUctZftr6nVWExJIqIACOhALiwgkqJe5mzXpRs+l3T5vcl5Lz+YPYGaaIYgoMfRX9e0FwAdRnFzGoHsbdW ZQwFr0GTUNkWM1UEA99i4rItMND+iMPgqMegJLqX0uZYY1xpSHLWOMtDrwTjbLFs8cWZEoNA2LLCY1hKIVEmRUuU8ske8qjLVlWVJk7JLkkxNXk5WNTOq1WabEdsnTxB1PQAMzlIOhqCofIVWTQiN4cKuSe5Z7hz2BBOrGIRV1PXQQerbO5qZPrxql28wsyASzon+vrmGzaEXMft2je8uNXzGpzFKxAAFtcgX/Kq+Nm42bZ6MPzRz9eYDi3cG3me 9Ahwzy4FQO2H+pArgNvQKyS+QuIAi6iOLyX0vwiy+hYO9+n9R2/wnYA/A06thgOIBAz7kAF2wM5MupiuH8OEeI6R8jlHqO0fo4e49p7iESCV2LiXY61fMA17KahtD6GsCfagF9KaOFcN3rwn9IWoX1ZRjBBA1QhWQeQoVeYWfMLE2whqRMt41QDaQ1o3AhxRvEuqxNslWxDhTFRO8WEepvhuJgFAb4HQBhGnuMQT4uxRo0o20LZb7HVvtYirpJjY Stv4529EvbfL4k8MSUd8TqBJMuSxbbUEXlikKYVNcpM8RUzpiyct376qtP12+79/2TTAcShM5PsznSbUSh6dwuq5sVO2YAiXiNGmC6EWG+XpwmW9U82xz9R8220DWiQCwa2C0eVC12XCxbh7EOX2hi1hSMweX2CYnxE0GJBumtHwGy3hlywp3y1jUKz+jtiSGmBTWHiJxZ3HjBThlQOT1KEC2gCa3EQJlFGYOawRS614DKgKQfHGBcywyGzpihGr xJUw3OQaRIIQDIIoOpU8VnzpTlnNH8QnyCQMJCVHyW183ZF2yE35TXzE3shOwlF1gKWTAhyLDCgbEImGXuwCg1AVT1GlUPxe3Umvw+01U7S3Af3+2aSB2NR4WvG0DvGALCkxTvCWG6Tj0pDTi7niBAKlVTD1DR2PAxx7BeVmRxwqwsOnEJzgMgB2W3AwKgPekpwKx7nvDFByPa0Z2DTxwZ1Z04NfRmRnlzUXmXlXmyF503gFwHW93QHeDCFIFQCX TV2bVbS2DmKYEWOt2WOmKgCNwkF13HVEMnSAW12N1nXNy/hnit3gSYkb0mGb1b0mHb1cU72717370H1wSPX8C9zbXWIWKWMFAuNDwfQjyYVIBYRjw/Ssx4W/Vq0EQazTwz2Azz2xjg1RPEOkMmCSCVHTBxU0TxXhF0SJWULrywyYh4AAGlnBJAYAeIOAOgpJmJ3gAA1DgCST4ZLdsG6Fkwkf4ebMw+fJSQw4HKffxBbMfCwrlUyaw1fUTJJewrfU 7AKcYaYbQe8MZDMWVZcNUw0O2TqCqK/N7DTd2C4mpb2H7Y0v7PVAHA1F/UU9/MqT/WUb/XeGzVqBHMOMUJzVyZ8Qor1DzWUMo2AnognATVAJAlGFArZWoknZuA5KLHAvg05eLQgrYQgd4TAfAVxZiUgKgZ6agpo2gsYegroCYO2Fgio9gjNVmcFeE/9VPFM0QoQwqP05suRLCBMdUDFW8fE5DbYVEJQ2vdnck9MzM7M3MqgbQxjRSSUoIq0IwiWZ bCU8w+A6UnlWUfbGw+UjfRU7fFUsUbQCmHEo0M2cA/6DobwsA5MMA+2Q0kw0pX2d7U0sI2pCIq0x/QzFpVbVyCOIsApcYCYMUGYdUGHXeOHJST01AfItMLyKYf04otuIMrzKshfBA8Mtg2MsLUnTApC0oN5Fo+NNossVCSspndNYcgYleIYrnHPAtcYotEtTXGYiAF+dYVAfQawGADih0C48gFtBXNi4gDiriniwIZ+LXEBEdLaPXI4g3PY9AE3M 3T0S3JdFdKkmkukhkpk1k9kzk7k3k/kj3H4/AVYiQISkSjgbi/QXiqhe9cPWiqPCEqi9haE7hBPeslPE5AQ3oFrSkB8DEzsnkBYNMA0oqPsivC4IctnKi7DCAQkQEUgT4eIFkxYfQAU3Q7jfQ+ctjUU9bTKzbbKtC9cwTXlOJETdfEPTffcxUbqZMM/aYa1e80oRTRqeqMKECiC6fG0K0m/M0+/D8qI5/YzYHTyNUjq+OdIz9VOAAsYf6JYHI/8h CyZEorHOZcigNKojCmor0OMiNcnIsmNEs1opzRqnIsi7o5nQePozNMkxEGiyPJeeiqACY/nWqHYrYOeZJVASEHwQBABSROXASlir6+CH6uAP6vAAGrgHYhShnBAC4fApoJgY4w3U4kMfkFSq4tS3HK6iAPBEysy9AUG4IcGyGs8ZCO9MPR9CoUlNhd9DInkOEqgsAN6bYCG9ETuBaQoaAbMTILYd+fMHoBgQgBACgDtN8qua/C4GW2WhEL+EQXKd sU4fQdEYI3q0I4W7ARWraZWjICWi0yIm06Inm7W0gJWlWwEEfAquchW823WlWtWhcvK02nW7IPW1W8UwU7Ku2i2jIWERfcMqmX2h2jIZiFfCqmyEO92y2wtPnJi0oM2v2/QEjHNRyp612+2mOjIVtOG4IRG+WpO0Oz2nKXY82tgCgbMXAXGrWt2qAD2qEdYF+cuyukIJiHKAkKcnm5gbAAkFEdxQqS1YKeNEAyVdJEQysXuu0fAAATWXAIiPKvGt RzmVFcj+QgCMDYAMG5vQgIBYS/TClq2jvrpVoDvQu7Bgi3Hlr9BIGGO51KBvstKfIjIlBujtCYidGtCNC/q/s9HoWUHHBymdChEOBAZAeyy2WPqduJHDuhu7DYIgDgECDMGEGYFEiPWIDvo2uLmx3oRIiPVqB3tKCyFwE0GCAAmj3nSIDgG4EodlBPX5ufWcr5BoXYVoecqPrsAknTxyFRBPTgHuDYA2EbtIfIf6PAARhXmRCbRWRelXCAA= %%","title":"Drawing"}]}